{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in data\n",
    "# data = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/FileStore/tables/enxedclg1508119193983/movies729.csv\")\n",
    "data = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"movies729.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>release_date</th>\n",
       "      <th>mpaa</th>\n",
       "      <th>runtime</th>\n",
       "      <th>rating</th>\n",
       "      <th>genres</th>\n",
       "      <th>number_reviews</th>\n",
       "      <th>production_budget</th>\n",
       "      <th>opening_weekend</th>\n",
       "      <th>gross</th>\n",
       "      <th>fb_likes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Star Wars: The Force Awakens</td>\n",
       "      <td>18-Dec-15</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>136</td>\n",
       "      <td>8.1</td>\n",
       "      <td>Action,Adventure,Fantasy,Sci-Fi</td>\n",
       "      <td>670255</td>\n",
       "      <td>245000000</td>\n",
       "      <td>247966675</td>\n",
       "      <td>936627416</td>\n",
       "      <td>267000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Wedding Ringer</td>\n",
       "      <td>16-Jan-15</td>\n",
       "      <td>R</td>\n",
       "      <td>101</td>\n",
       "      <td>6.7</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>59500</td>\n",
       "      <td>23000000</td>\n",
       "      <td>24500000</td>\n",
       "      <td>64460211</td>\n",
       "      <td>74000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Visit</td>\n",
       "      <td>11-Sep-15</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>94</td>\n",
       "      <td>6.2</td>\n",
       "      <td>Horror,Thriller</td>\n",
       "      <td>83345</td>\n",
       "      <td>5000000</td>\n",
       "      <td>25427560</td>\n",
       "      <td>65069140</td>\n",
       "      <td>29000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Magic Mike XXL</td>\n",
       "      <td>1-Jul-15</td>\n",
       "      <td>R</td>\n",
       "      <td>115</td>\n",
       "      <td>5.7</td>\n",
       "      <td>Comedy,Drama,Music</td>\n",
       "      <td>42965</td>\n",
       "      <td>14800000</td>\n",
       "      <td>11600000</td>\n",
       "      <td>66009973</td>\n",
       "      <td>41000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>War Room</td>\n",
       "      <td>28-Aug-15</td>\n",
       "      <td>PG</td>\n",
       "      <td>120</td>\n",
       "      <td>6.4</td>\n",
       "      <td>Drama</td>\n",
       "      <td>10010</td>\n",
       "      <td>3000000</td>\n",
       "      <td>11351389</td>\n",
       "      <td>67790117</td>\n",
       "      <td>30000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          title release_date   mpaa runtime rating  \\\n",
       "0  Star Wars: The Force Awakens    18-Dec-15  PG-13     136    8.1   \n",
       "1            The Wedding Ringer    16-Jan-15      R     101    6.7   \n",
       "2                     The Visit    11-Sep-15  PG-13      94    6.2   \n",
       "3                Magic Mike XXL     1-Jul-15      R     115    5.7   \n",
       "4                      War Room    28-Aug-15     PG     120    6.4   \n",
       "\n",
       "                            genres number_reviews production_budget  \\\n",
       "0  Action,Adventure,Fantasy,Sci-Fi         670255         245000000   \n",
       "1                           Comedy          59500          23000000   \n",
       "2                  Horror,Thriller          83345           5000000   \n",
       "3               Comedy,Drama,Music          42965          14800000   \n",
       "4                            Drama          10010           3000000   \n",
       "\n",
       "  opening_weekend      gross fb_likes  \n",
       "0       247966675  936627416   267000  \n",
       "1        24500000   64460211    74000  \n",
       "2        25427560   65069140    29000  \n",
       "3        11600000   66009973    41000  \n",
       "4        11351389   67790117    30000  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at our data in dataframe form\n",
    "# function to select a few rows of data, convert to a Pandas dataframe, and transpose\n",
    "def preview(df, n=5):\n",
    "    return pd.DataFrame(df.take(n), columns=df.columns)\n",
    "\n",
    "preview(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|     mpaa|count|\n",
      "+---------+-----+\n",
      "|        R|  306|\n",
      "|    PG-13|  278|\n",
      "|       PG|  103|\n",
      "|Not Rated|   23|\n",
      "|        G|   10|\n",
      "|  Unrated|    8|\n",
      "|    NC-17|    1|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# practice using sql queries in Spark\n",
    "# some EDA with mpaa counts\n",
    "\n",
    "data.registerTempTable('data')\n",
    "mpaa_count = spark.sql(r\"\"\"SELECT mpaa, COUNT(title) AS count\n",
    "                      FROM data\n",
    "                      GROUP BY mpaa\n",
    "                      ORDER BY count DESC\"\"\")\n",
    "mpaa_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "1. Drop columns that are not eventually going into the predictive model\n",
    "2. Cast the columns into the appropriate data type\n",
    "3. Vectorize our features\n",
    "4. Transform and scale our features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# columns not used in analysis\n",
    "for field in ['title', 'release_date', 'mpaa', 'runtime', 'genres','number_reviews']:\n",
    "    data = data.drop(field)\n",
    "\n",
    "# convert numerical value strings to double    \n",
    "for column in data.schema.names:\n",
    "    data = data.withColumn(column, data[column].cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "train, test = data.randomSplit([0.7, 0.3], seed=5555)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Spark, before running the model, we need to combine the features into one vector. This can be done using the VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|       gross|            features|\n",
      "+------------+--------------------+\n",
      "|   9353573.0|[2.6,3.0E7,636887...|\n",
      "| 7.4158157E7|[3.4,7.9E7,2.5003...|\n",
      "| 2.5615792E7|[3.5,2.5E7,1.2622...|\n",
      "|   8742261.0|[3.7,1.8E7,472111...|\n",
      "|      3506.0|[3.9,650000.0,70....|\n",
      "|     14686.0|[4.1,1000000.0,86...|\n",
      "|   4930798.0|[4.1,1.7E7,295500...|\n",
      "|1.66147885E8|[4.1,4.0E7,9.4395...|\n",
      "| 7.9566871E7|[4.3,3.0E7,3.2324...|\n",
      "| 9.5328937E7|[4.3,1.0E8,937255...|\n",
      "+------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# the feature columns\n",
    "features = [f for f in data.schema.names if f != 'gross']\n",
    "\n",
    "assembler = VectorAssembler(inputCols=features, outputCol='features')\n",
    "\n",
    "train_pack = assembler.transform(train)\n",
    "test_pack = assembler.transform(test)\n",
    "\n",
    "# remove the columns packed into the feature vector\n",
    "for field in features:\n",
    "    train_pack = train_pack.drop(field)\n",
    "    test_pack  = test_pack.drop(field)\n",
    "\n",
    "# see what's returned\n",
    "train_pack.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "standardizing our features so that they have 0 mean and unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|       gross|     features_scaled|\n",
      "+------------+--------------------+\n",
      "|   9353573.0|[-4.3720155579867...|\n",
      "| 7.4158157E7|[-3.4773744484751...|\n",
      "| 2.5615792E7|[-3.3655443097862...|\n",
      "|   8742261.0|[-3.1418840324083...|\n",
      "|      3506.0|[-2.9182237550304...|\n",
      "|     14686.0|[-2.6945634776525...|\n",
      "|   4930798.0|[-2.6945634776525...|\n",
      "|1.66147885E8|[-2.6945634776525...|\n",
      "| 7.9566871E7|[-2.4709032002747...|\n",
      "| 9.5328937E7|[-2.4709032002747...|\n",
      "+------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# standardizing features\n",
    "\n",
    "standardscale = StandardScaler(withMean=True, withStd=True, inputCol='features', outputCol='features_scaled')\n",
    "standardscale = standardscale.fit(train_pack)\n",
    "\n",
    "train_pack = standardscale.transform(train_pack)\n",
    "train_pack = train_pack.drop('features')\n",
    "test_pack  = standardscale.transform(test_pack)\n",
    "test_pack = test_pack.drop('features')\n",
    "\n",
    "train_pack.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am also going to add two other columns to my Spark dataframe: \n",
    "1. the log transformed gross \n",
    "2. standardized gross "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import log, mean, stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize(df, column):\n",
    "  average = df.agg(mean(df[column]).alias(\"mean\")).collect()[0][\"mean\"]\n",
    "  std = df.agg(stddev(df[column]).alias(\"std\")).collect()[0][\"std\"]\n",
    "  return (df[column] - average) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+\n",
      "|     features_scaled|         log_gross|        gross_scaled|\n",
      "+--------------------+------------------+--------------------+\n",
      "|[-4.3720155579867...|16.051268967305777| -0.6324265720878373|\n",
      "|[-3.4773744484751...|18.121710627241004| 0.09077202318335206|\n",
      "|[-3.3655443097862...| 17.05871959426062|-0.45094536796819584|\n",
      "|[-3.1418840324083...| 15.98367940982794| -0.6392486200416119|\n",
      "|[-2.9182237550304...|  8.16223106548118| -0.7367703530785693|\n",
      "|[-2.6945634776525...| 9.594649938011555| -0.7366455878259272|\n",
      "|[-2.6945634776525...|15.411011399045726| -0.6817833377635575|\n",
      "|[-2.6945634776525...|18.928388823222527|  1.1173481997849761|\n",
      "|[-2.4709032002747...|18.192108370714735| 0.15113155526558278|\n",
      "|[-2.4709032002747...|18.372843963668053|  0.3270312106012574|\n",
      "+--------------------+------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# log transforming target\n",
    "train_pack = train_pack.withColumn('log_gross', log(train_pack.gross))\n",
    "test_pack = test_pack.withColumn('log_gross', log(test_pack.gross))\n",
    "\n",
    "\n",
    "# standardizing target\n",
    "# dropping original gross column\n",
    "train_pack = train_pack.withColumn('gross_scaled', standardize(train_pack,'gross'))\n",
    "train_pack = train_pack.drop('gross')\n",
    "\n",
    "test_pack = test_pack.withColumn('gross_scaled', standardize(test_pack,'gross'))\n",
    "test_pack = test_pack.drop('gross')\n",
    "\n",
    "\n",
    "train_pack.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling.\n",
    "\n",
    "I will build a predicitive model of gross revenue using linear regression.\n",
    "\n",
    "I will try two models and see which one predicts better:\n",
    "1. log_gross as the response variable\n",
    "2. gross_scaled as a the response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression(labelCol='log_gross', \n",
    "                        featuresCol='features_scaled',\n",
    "                        predictionCol='prediction')\n",
    "\n",
    "lr_model = lr.fit(train_pack)\n",
    "# transform ~ predict\n",
    "lr_pred = lr_model.transform(test_pack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.3350991076008999\n",
      "MSE: 4.991448705041381\n",
      "\n",
      "\n",
      "Estimated Coefficeint:\n",
      "[('rating', 0.1771739881851411),\n",
      " ('production_budget', 0.87076304274451455),\n",
      " ('opening_weekend', 0.68590296489508062),\n",
      " ('gross', 0.37035362207759798)]\n"
     ]
    }
   ],
   "source": [
    "print('R2:',lr_model.summary.r2)\n",
    "print('MSE:',lr_model.summary.meanSquaredError)\n",
    "print('\\n')\n",
    "print('Estimated Coefficeint:')\n",
    "pprint(list(zip(data.schema.names, lr_model.coefficients)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model isn't really predicitive at all. Let's try again with the standardized gross value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr2 = LinearRegression(labelCol='gross_scaled', \n",
    "                        featuresCol='features_scaled',\n",
    "                        predictionCol='prediction')\n",
    "\n",
    "lr2_model = lr2.fit(train_pack)\n",
    "# transform ~ predict\n",
    "lr2_pred = lr2_model.transform(test_pack)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.7564348554482099\n",
      "MSE: 0.24309220252353414\n",
      "\n",
      "\n",
      "Estimated Coefficeint:\n",
      "[('rating', 0.11788177782631756),\n",
      " ('production_budget', 0.2570232449241947),\n",
      " ('opening_weekend', 0.65400579259557645),\n",
      " ('gross', 0.043520333116376868)]\n"
     ]
    }
   ],
   "source": [
    "print('R2:',lr2_model.summary.r2)\n",
    "print('MSE:',lr2_model.summary.meanSquaredError)\n",
    "print('\\n')\n",
    "print('Estimated Coefficeint:')\n",
    "pprint(list(zip(data.schema.names, lr2_model.coefficients)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "name": "test",
  "notebookId": 916178359856536
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
